{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "6c5004db-718e-4f23-90af-9ce974bf8509",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1647899455366,
    "source_hash": "8ad57f32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start writing code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "b76bd9d6-e913-467a-9195-f27db7d97469",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1647899455376,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cell_id": "f08f554a-07cc-4457-95a1-659776eda2da",
    "deepnote_cell_height": 716,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4001,
    "execution_start": 1647901543209,
    "source_hash": "2712107c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in /root/venv/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: wcwidth in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from prettytable) (0.2.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from prettytable) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (4.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import random, nltk, re, math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "from nltk import ngrams, bigrams, trigrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader, gutenberg\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from decimal import Decimal\n",
    "from math import log, exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d7bd150c-d0e9-48d7-9934-5a0cd3b6e01f",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "6863eb6d-552d-4704-8bdb-b9a144b52e42",
    "deepnote_cell_height": 513,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1647899500901,
    "source_hash": "db4d030",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNIGRAM\n",
    "def word_frequency(corpus):\n",
    "    \"\"\"Calculate the frequency/bigram of each words in the input-text\n",
    "    Input: corpus\n",
    "    Output: dictionary of words and frequency ratio, and dictionary of words and count \"\"\"\n",
    "\n",
    "    #to list of tokens/words\n",
    "    text = [w for w in corpus.words()]\n",
    "\n",
    "    # to remove special characters and empty\n",
    "    text = list(filter(None, [re.sub(r'[^a-zA-Z0-9]','',string) for string in text]))\n",
    "\n",
    "    #initiate a counter and word frequency\n",
    "    word_count = Counter(text)\n",
    "    word_freq = Counter(text)\n",
    "\n",
    "    #add \n",
    "    for w in word_freq:\n",
    "        word_freq[w] /= float(len(text))\n",
    "    \n",
    "    #sum of frequencies/probabilities must total to 1.\n",
    "    if sum(word_freq.values()) > 1.01 and sum(word_freq.values()) < 0.98:\n",
    "        return print('Error: the sum of frequencies is more than 1.')\n",
    "\n",
    "    return word_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d0a07ab4-09a0-4e78-b0d9-29daa7aaa644",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "6654e64d-ce45-452d-80c7-5068ce57fe3e",
    "deepnote_cell_height": 531,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1647899554124,
    "source_hash": "4b533efc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BIGRAM\n",
    "def create_bigram(corpus):\n",
    "    \"\"\" from a given text corpus, finds the conditional probability of a word occuring and returns \n",
    "    a dicitonary from the bigram with associated probabilities as well as a table of the bigrams \"\"\"\n",
    "\n",
    "    model_dict= defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    tab = PrettyTable()\n",
    "    tab.title = \"bigrams for the sample's first sentences\"\n",
    "    tab.field_names = ['#1','#2']\n",
    "\n",
    "    for sentence in corpus.sents():\n",
    "        sentence = list(filter(None, [re.sub(r'[^a-zA-Z0-9]','',str_element) for str_element in sentence]))\n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            tab.add_row((w1, w2))\n",
    "            model_dict[(w1)][w2] += 1\n",
    "\n",
    "    for w1_w2 in model_dict:\n",
    "        total_count = float(sum(model_dict[w1_w2].values()))\n",
    "        for w2 in model_dict[w1_w2]:\n",
    "            if total_count == 0:\n",
    "                model_dict[w1_w2][w2] = 0\n",
    "            else:\n",
    "                model_dict[w1_w2][w2] /= total_count\n",
    "            \n",
    "    return model_dict, tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bbe41420-8f65-4771-9e5f-68be3710020a",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "864568ae-ebec-41cd-a98f-7b1209bb041e",
    "deepnote_cell_height": 495,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1647899555972,
    "source_hash": "520f3d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TRIGRAM\n",
    "def create_trigram(corpus):\n",
    "\n",
    "    model_dict= defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    tab = PrettyTable()\n",
    "    tab.title = \"trigrams for the sample's first sentences\"\n",
    "    tab.field_names = ['#1','#2', '#3']\n",
    "\n",
    "    for sentence in corpus.sents():\n",
    "        sentence = list(filter(None, [re.sub(r'[^a-zA-Z0-9]','',str_element) for str_element in sentence]))\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            tab.add_row((w1, w2, w3))\n",
    "            model_dict[(w1, w2)][w3] += 1\n",
    "\n",
    "    for w1_w2 in model_dict:\n",
    "        total_count = float(sum(model_dict[w1_w2].values()))\n",
    "        for w3 in model_dict[w1_w2]:\n",
    "            if total_count == 0:\n",
    "                model_dict[w1_w2][w3] = 0\n",
    "            else:\n",
    "                model_dict[w1_w2][w3] /= total_count\n",
    "    \n",
    "    return model_dict, tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "4dae5d4b-b51a-4aca-9337-e8c2df10fa91",
    "deepnote_cell_height": 1115,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 60416,
    "execution_start": 1647899557819,
    "source_hash": "4e792d27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- unigram output -----------------------\n",
      "\n",
      "probability of the word 'Emma' is 0.0003937790190355324\n",
      "{'Emma': 0.0003937790190355324, 'by': 0.0036431379913541406, 'Jane': 0.00013777718564407198, 'Austen': 1.3641305509314055e-06, '1816': 4.5471018364380185e-07}\n",
      "\n",
      "------------------- bigram output -----------------------\n",
      "\n",
      "probability of the word 'world' given 'the' is 0.006640159045725646\n",
      "probability of the word 'lived' given 'had' is 0.0015710919088766694\n",
      "probability of the word 'the' given 'None' is 0.0017258357613473701\n",
      "+---------------------------------------+\n",
      "| bigrams for the sample's first sentences |\n",
      "+-------------------+-------------------+\n",
      "|         #1        |         #2        |\n",
      "+-------------------+-------------------+\n",
      "|        None       |        Emma       |\n",
      "|        Emma       |         by        |\n",
      "|         by        |        Jane       |\n",
      "|        Jane       |       Austen      |\n",
      "|       Austen      |        1816       |\n",
      "|        1816       |        None       |\n",
      "|        None       |       VOLUME      |\n",
      "|       VOLUME      |         I         |\n",
      "|         I         |        None       |\n",
      "|        None       |      CHAPTER      |\n",
      "|      CHAPTER      |         I         |\n",
      "|         I         |        None       |\n",
      "|        None       |        Emma       |\n",
      "|        Emma       |     Woodhouse     |\n",
      "|     Woodhouse     |      handsome     |\n",
      "|      handsome     |       clever      |\n",
      "|       clever      |        and        |\n",
      "|        and        |        rich       |\n",
      "|        rich       |        with       |\n",
      "|        with       |         a         |\n",
      "+-------------------+-------------------+\n",
      "\n",
      "------------------- trigram output -----------------------\n",
      "\n",
      "probability of the word 'world' given 'in' and 2 is 0.0247732\n",
      "probability of the word 'lived' given 'had' and 2 is 0.0000000\n",
      "probability of the word 'the' given 'None' and 2 is 0.0388657\n",
      "+--------------------------------------------+\n",
      "| trigrams for the sample's first sentences  |\n",
      "+--------------+--------------+--------------+\n",
      "|      #1      |      #2      |      #3      |\n",
      "+--------------+--------------+--------------+\n",
      "|     None     |     None     |     Emma     |\n",
      "|     None     |     Emma     |      by      |\n",
      "|     Emma     |      by      |     Jane     |\n",
      "|      by      |     Jane     |    Austen    |\n",
      "|     Jane     |    Austen    |     1816     |\n",
      "|    Austen    |     1816     |     None     |\n",
      "|     1816     |     None     |     None     |\n",
      "|     None     |     None     |    VOLUME    |\n",
      "|     None     |    VOLUME    |      I       |\n",
      "|    VOLUME    |      I       |     None     |\n",
      "|      I       |     None     |     None     |\n",
      "|     None     |     None     |   CHAPTER    |\n",
      "|     None     |   CHAPTER    |      I       |\n",
      "|   CHAPTER    |      I       |     None     |\n",
      "|      I       |     None     |     None     |\n",
      "|     None     |     None     |     Emma     |\n",
      "|     None     |     Emma     |  Woodhouse   |\n",
      "|     Emma     |  Woodhouse   |   handsome   |\n",
      "|  Woodhouse   |   handsome   |    clever    |\n",
      "|   handsome   |    clever    |     and      |\n",
      "+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "word_frequencies, word_counts = word_frequency(gutenberg)\n",
    "print('\\n------------------- unigram output -----------------------\\n')\n",
    "print(\"probability of the word '{0}' is {1}\".format('Emma', word_frequencies['Emma']))\n",
    "from itertools import islice\n",
    "print(dict(islice(word_frequencies.items(), 0, 5)))\n",
    "\n",
    "\n",
    "bigram_model_gutenberg, bigram_tab_gutenberg = create_bigram(gutenberg)\n",
    "print('\\n------------------- bigram output -----------------------\\n')\n",
    "print(\"probability of the word '{0}' given '{1}' is {2}\".format('world', 'the', bigram_model_gutenberg[\"the\"][\"world\"]))\n",
    "print(\"probability of the word '{0}' given '{1}' is {2}\".format('lived', 'had', bigram_model_gutenberg[\"had\"][\"lived\"]))\n",
    "print(\"probability of the word '{0}' given '{1}' is {2}\".format('the', 'None', bigram_model_gutenberg[None][\"the\"]))\n",
    "\n",
    "print(bigram_tab_gutenberg[:20])\n",
    "\n",
    "\n",
    "trigram_model_gutenberg, trigram_tab_gutenberg = create_trigram(gutenberg)\n",
    "print('\\n------------------- trigram output -----------------------\\n')\n",
    "print(\"probability of the word '{0}' given '{1}' and 2 is {3}\".format('world', 'in','the',\"{:.7f}\".format( trigram_model_gutenberg[\"in\", \"the\"][\"world\"])))\n",
    "print(\"probability of the word '{0}' given '{1}' and 2 is {3}\".format('lived','had','nearly',\"{:.7f}\".format(trigram_model_gutenberg[\"had\", \"nearly\"][\"lived\"])))\n",
    "print(\"probability of the word '{0}' given '{1}' and 2 is {3}\".format('the','None','None',\"{:.7f}\".format(trigram_model_gutenberg[None, None][\"The\"])))\n",
    "\n",
    "print(trigram_tab_gutenberg[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "50978403-e79e-47af-a924-3bd89be9eb42",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cell_id": "5f022dd8-5355-410e-9f9e-792527fc4214",
    "deepnote_cell_height": 908,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 48722,
    "execution_start": 1647901494438,
    "source_hash": "a2355321",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of sentence 1st being in the input corpus is: 4.7060609797916105e-06\n",
      "The probability of sentence 5th being in the input corpus is: 7.082434712502128e-63\n",
      "The probability of sentence 10th being in the input corpus is: 2.0509775328704013e-26\n"
     ]
    }
   ],
   "source": [
    "def interpolate(unigram, bigram, trigram, counts, lambda_list, corpus):\n",
    "    \"\"\" Finds a interpolated probabilites of each sentence in text. \n",
    "\n",
    "    Input: text corpus and additionally a trained unigram, bigram, trigram, \n",
    "            counts and list of three lambda values in range 0-1. \n",
    "            Otherwise calculates the n-grams and default linear interpolation\n",
    "    Output: list of probabilities for sentences in input corpus\"\"\"\n",
    "\n",
    "    # calculate n-grams if not given as arguments\n",
    "    if (unigram is None): unigram, counts = word_frequency(corpus)\n",
    "    if (unigram is None): bigram, bitab = create_bigram(corpus)\n",
    "    if (trigram is None): trigram, tritab = create_trigram(corpus)\n",
    "\n",
    "    probs_list = []\n",
    "\n",
    "    #linear interpolation if no other values are given\n",
    "    if (lambda_list is None): \n",
    "        lambda1, lambda2, lambda3 = 1.0/3.0, 1.0/3.0, 1.0/3.0\n",
    "    else:\n",
    "        lambda1, lambda2, lambda3 = lambda_list[0], lambda_list[1], lambda_list[2]\n",
    "\n",
    "    #prepares sentences\n",
    "    for sentence in corpus.sents():\n",
    "        sentence = list(filter(None, [re.sub(r'[^a-zA-Z0-9]','',str_element) for str_element in sentence]))\n",
    "        trigram_sentence = list(trigrams(sentence, pad_left=True, pad_right=True))\n",
    "        probability_of_sentence = 1.0\n",
    "\n",
    "        #product of probabilities for each sentence\n",
    "        for w1,w2,w3 in trigram_sentence:\n",
    "            if counts[w3] > 0:\n",
    "                prob_tri, prob_bi, prob_uni = trigram[w1, w2][w3], bigram[w2][w3], unigram[w3]\n",
    "                probability_of_sentence *= lambda1*prob_tri + lambda2*prob_bi + lambda3*prob_uni\n",
    "\n",
    "        probs_list.append(probability_of_sentence)\n",
    "\n",
    "    return probs_list\n",
    "\n",
    "interpolated_probabilities = interpolate(word_frequencies, bigram_model_gutenberg, trigram_model_gutenberg, word_counts, None, gutenberg)\n",
    "interpolated_probabilities2 = interpolate(word_frequencies, bigram_model_gutenberg, trigram_model_gutenberg, word_counts, [0.1, 0.2, 0.7], gutenberg)\n",
    "\n",
    "print('The probability of sentence {0} being in the input corpus is: {1}'.format('1st', (interpolated_probabilities[1])))\n",
    "print('The probability of sentence {0} being in the input corpus is: {1}'.format('5th', (interpolated_probabilities[5])))\n",
    "print('The probability of sentence {0} being in the input corpus is: {1}'.format('10th', (interpolated_probabilities[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38a726b4-3ca4-456d-b7a4-d9febbb056cc",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Maximise probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cell_id": "94eef66c-50a0-454f-a76a-71d8149da7de",
    "deepnote_cell_height": 747,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1647902292778,
    "source_hash": "4c28cc95",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximum_probability_model(unigram, bigram, trigram, counts, corpus):\n",
    "    \"\"\" Finds random lambda values and saves probabilies from interpolate() function. \n",
    "    Input: text corpus, and optionally a unigram, bigram, trigram, counts\n",
    "    Output: dictionary of lambda combinations and score from magnitude of probabilities\n",
    "    \"\"\"\n",
    "    lambda1, lambda2, lambda3 = [], [], []\n",
    "\n",
    "    #generate random lambdas\n",
    "    random.seed(32)\n",
    "    for i in range(20):\n",
    "        random1, random2  = round(random.uniform(0, 1),1), round(random.uniform(0, 1),1)\n",
    "        if random2 < (1 - random1):\n",
    "            lambda1.append(random1), lambda2.append(random2), lambda3.append(round((1 - random1)-random2,1))\n",
    "\n",
    "\n",
    "    score_dict = {} # key as combinations of lambdas and values as probability magnitude\n",
    "\n",
    "    #getting the probability score for each combination of random lambdas\n",
    "    for i in range(len(lambda1)):\n",
    "        lambdas = [lambda1[i], lambda2[i], lambda3[i]]\n",
    "        k = ','.join(map(str, lambdas)) #generates key for dict\n",
    "        if k not in score_dict:\n",
    "            probabilities = interpolate(word_frequencies, bigram_model_gutenberg, trigram_model_gutenberg, word_counts, lambdas, gutenberg)\n",
    "            avg_prob = sum(probabilities)/len(probabilities)\n",
    "            score_dict[k] = avg_prob\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "def get_optimal_lambdas(score_dict):\n",
    "    \"\"\"from input dictionary of lambda combinations and probability score, \n",
    "    returns the key with maximum probability score \"\"\"\n",
    "\n",
    "    max_lambdas = max(score_dict, key=score_dict.get)\n",
    "    max_prob = score_dict[max_lambdas]\n",
    "\n",
    "    return max_lambdas, max_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_id": "5ec68a17-5a7a-45f2-9e32-87d6af2d916c",
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 207169,
    "execution_start": 1647902296512,
    "source_hash": "83281361",
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_dict = maximum_probability_model(word_frequencies, bigram_model_gutenberg, trigram_model_gutenberg, word_counts, gutenberg)\n",
    "opt_lambda, opt_score = get_optimal_lambdas(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cell_id": "24af276ffb8f48988aa3523e833747dc",
    "deepnote_cell_height": 346,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1647902528365,
    "source_hash": "e8efac11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal combination of lambda is: 0.8,0.1,0.1 with average probability0.0010263715293983948\n",
      " For lambdas 0.1,0.2,0.7 average probability is 0.0009772861404375428\n",
      " For lambdas 0.1,0.5,0.4 average probability is 0.0010012735926912607\n",
      " For lambdas 0.4,0.0,0.6 average probability is 0.0009856610552172662\n",
      " For lambdas 0.8,0.1,0.1 average probability is 0.0010263715293983948\n",
      " For lambdas 0.0,0.7,0.3 average probability is 0.0010091968823131693\n",
      " For lambdas 0.1,0.1,0.8 average probability is 0.0009693307705192458\n",
      " For lambdas 0.2,0.0,0.8 average probability is 0.0009694690803112595\n",
      " For lambdas 0.0,0.0,1.0 average probability is 0.0009533332748930593\n",
      " For lambdas 0.4,0.2,0.4 average probability is 0.0010016506577498429\n"
     ]
    }
   ],
   "source": [
    "print('Optimal combination of lambda is: {0} with average probability{1}/n'.format(opt_lambda, opt_score))\n",
    "\n",
    "for key, value in score_dict.items():\n",
    "    print('For lambdas {0} average probability is {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "368bd84b53ed4903b4dd239f6d0950a7",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Randomly generated text from trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "03eb2614-796a-4aa4-a4d0-fdf466b5c1e6",
    "deepnote_cell_height": 730,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1,
    "execution_start": 1647893212849,
    "source_hash": "f40fc471",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text= 9.19625508578272e-62\n",
      "Well it looks at in all Macedonia but we were among the tribes from the Bashee isles we emerged at last it sat for some time though to his keeping his bullet and had almost told her how to be the king of creation no primal solitude Dense joyous modern populous millions cities and dwelt in the world to be a book\n"
     ]
    }
   ],
   "source": [
    "def generate_text_trigram(trigram_model):\n",
    "    \"\"\"given a trigram model as input, returns a randomly generated text and \n",
    "    its probability \"\"\"\n",
    "\n",
    "    text = [None, None]\n",
    "    prob = 1.0  # <- Init probability\n",
    "    \n",
    "    sentence_finished = False\n",
    "    while not sentence_finished:\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        #iterating words in given trigram\n",
    "        for word in trigram_model[tuple(text[-2:])].keys():\n",
    "            accumulator += trigram_model[tuple(text[-2:])][word]\n",
    "\n",
    "            #choose selected word based on random threshold\n",
    "            if accumulator >= r:\n",
    "                prob *= trigram_model[tuple(text[-2:])][word]  # <- Update the probability with the conditional probability of the new word\n",
    "                text.append(word)\n",
    "                break\n",
    "    \n",
    "        #stop argument\n",
    "        if text[-2:] == [None, None]:\n",
    "            sentence_finished = True\n",
    "\n",
    "    return text, prob\n",
    "\n",
    "generated_text, prob_gt = generate_text_trigram(trigram_model_gutenberg)\n",
    "print(\"Probability of text=\", prob_gt)  # <- Print the probability of the text\n",
    "print(' '.join([t for t in generated_text if t]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "465f36c2-d434-4eeb-bcfa-f8bc2e3373ed",
    "deepnote_cell_height": 186,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1,
    "execution_start": 1647893463317,
    "source_hash": "64ef15d8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text= 2.897215706541899e-17\n",
      "This man only nodded and was just wishing I had better not talk about wasting IT\n"
     ]
    }
   ],
   "source": [
    "generated_text, prob_gt = generate_text_trigram(trigram_model_gutenberg)\n",
    "print(\"Probability of text=\", prob_gt)  # <- Print the probability of the text\n",
    "print(' '.join([t for t in generated_text if t]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d44cbe547d2446c0abfc3754b734fbe8",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Question 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "690632b8-315b-429d-ba24-dbd5a1d467ee",
    "deepnote_cell_height": 171,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1647899947401,
    "source_hash": "b38759c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "Xtrain = [\"the actor gives a convincing, charismatic performance as the multifaceted\", \"Spielberg gives us a visually spicy and historically accurate real life story\", \"His innovative mind entertains us now and will continue to entertain generations to come\", \"Unfortunately, the film has two major flaws, one in the disastrous ending\", \"If director actually thought this movie was worth anything\", \"His efforts seem fruitless, creates drama where drama shouldn't be\"]\n",
    "ytrain = [\"entertaining\", \"entertaining\", \"entertaining\", \"boring\", \"boring\", \"boring\"]\n",
    "Xtest = \"film is a innovative drama, entertains, but disastrous ending\"\n",
    "categories = [\"entertaining\", \"boring\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "aeb73bbf1b6b4d188773dc2bd65c6c92",
    "deepnote_cell_height": 1608,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1647899955045,
    "source_hash": "470fb8ea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count in train data set corpus:  62\n",
      "Vocabulary of train data set corpus:  56\n"
     ]
    }
   ],
   "source": [
    "# step 1. calculate prior probabilities of sentences P(c) = Nc / total N\n",
    "def prob_counter(cat, labels):\n",
    "    \"\"\" as input takes list of categories and list of labels returns a dictioanary with \n",
    "    probabilities of appearance of each category \"\"\"\n",
    "    prob_dict = {}\n",
    "    for i in cat:\n",
    "        prob = labels.count(i)/len(labels)\n",
    "        #name each key as prob + name of the category\n",
    "        prob_dict[\"prob_\"+i] = prob\n",
    "    return prob_dict\n",
    "\n",
    "prob_counts = prob_counter(categories, ytrain)\n",
    "\n",
    "\n",
    "# step 2. calculate the vocabulary size\n",
    "# Search for negative expressions and change them on two words\n",
    "def neg_remover(train_set):\n",
    "    \"\"\" takes traning set and finds negative parts (n't or not) \n",
    "    to add not_ to each word after that before comma if there is\"\"\" \n",
    "    tokenized_fully = []\n",
    "    for i in range(len(train_set)):\n",
    "        \n",
    "        splitted = nltk.word_tokenize(train_set[i])\n",
    "        if \"not\" in splitted or \"n't\" in splitted:\n",
    "\n",
    "            if \"n't\" in splitted:\n",
    "                number = splitted.index(\"n't\")\n",
    "                splitted[splitted.index(\"n't\")] = \"not\"\n",
    "            elif \"not\" in splitted: \n",
    "                number = splitted.index(\"not\") \n",
    "        \n",
    "            for j in range(len(splitted)):\n",
    "                if j > number or j > number:\n",
    "                    if splitted[j] == \",\":\n",
    "                        break\n",
    "                    else:\n",
    "                        splitted[j] = \"not_\" + splitted[j]\n",
    "                  \n",
    "        tokenized_fully.append(splitted)\n",
    "    return tokenized_fully\n",
    "\n",
    "# remove commas\n",
    "def comma_remover(train_set):\n",
    "    \"\"\" input is tokenized corpus. Function removes commas\"\"\"\n",
    "    for i in train_set:\n",
    "        try:\n",
    "            while True:\n",
    "                i.remove(\",\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return train_set\n",
    "\n",
    "\n",
    "# remove the\n",
    "def the_remover(train_set):\n",
    "    \"\"\" input is tokenized corpus. Function removes \"the\" \"\"\"\n",
    "    for i in train_set:\n",
    "        try:\n",
    "            while True:\n",
    "                i.remove(\"the\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return train_set\n",
    "\n",
    "# remove a\n",
    "def a_remover(train_set):\n",
    "    \"\"\" input is tokenized corpus. Function removes \"a\" \"\"\"\n",
    "    for i in train_set:\n",
    "        try:\n",
    "            while True:\n",
    "                i.remove(\"a\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return train_set\n",
    "\n",
    "separ2 = a_remover(the_remover(comma_remover(neg_remover(Xtrain))))\n",
    "\n",
    "combined = sum(separ2, [])\n",
    "total_count = len(combined)\n",
    "print('Total count in train data set corpus: ', total_count)\n",
    "vocabulary = set(combined)\n",
    "length_voc = len(vocabulary)\n",
    "print('Vocabulary of train data set corpus: ', length_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "2d64dbdfd85a45c79f9a9bdaa0640102",
    "deepnote_cell_height": 346,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1647899956426,
    "source_hash": "8b69a238",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'innovative', 'drama', 'entertains', 'disastrous', 'ending']\n"
     ]
    }
   ],
   "source": [
    "# step 3. drop unknown words from the test set\n",
    "neg_remover([Xtest]) #check negative sentences to change words\n",
    "\n",
    "def word_removal(test_str):\n",
    "    \"\"\"  takes a test string to remove all words that does not appear in the test set \"\"\"\n",
    "    bag_words = []\n",
    "    test_tok = word_tokenize(Xtest)\n",
    "    for i in test_tok:\n",
    "        if i in vocabulary:\n",
    "            bag_words.append(i)\n",
    "    return bag_words\n",
    "\n",
    "final_test = word_removal(Xtest)\n",
    "print(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "62c84fe889c346c78407f890ee4ea40e",
    "deepnote_cell_height": 1107,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1647899958049,
    "source_hash": "bbb2b870",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 4. calculate given probabilities P(w|c) = count(w|c)+1 / counts(w+c) + vocab\n",
    "\n",
    "# create template\n",
    "def template(cat, bag_words):\n",
    "    \"\"\" input is a list with categories and unique words in a test set returns the dictionary \n",
    "    for further calculations of probabilities \"\"\"\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for i in bag_words:\n",
    "        for c in cat:\n",
    "            model[i][c] = 1 # add one for smoothing instead of 0\n",
    "    return model\n",
    "\n",
    "model_ent = template(categories, final_test)\n",
    "\n",
    "# count appearance \n",
    "def words_quantity(categories, bag_words, labels, t_train_set, model):\n",
    "    \"\"\" takes template model, categories, unique words in a test set, labels, and training test\n",
    "    to return the model with quantities of each word by category\"\"\"\n",
    "    for i in bag_words:\n",
    "        for cat in categories:\n",
    "            for sent in t_train_set:\n",
    "                if labels[t_train_set.index(sent)] == cat:\n",
    "                    for word in sent:\n",
    "                        if word == i:\n",
    "                            model[i][cat] += 1\n",
    "    return model\n",
    "\n",
    "model_ent = words_quantity(categories, final_test, ytrain, separ2, model_ent)\n",
    "\n",
    "# count number of words in each category\n",
    "def category_length(category, t_train_set, labels):\n",
    "    \"\"\" creates a dictionary where a key is length + category and value is number of words \n",
    "    in each category by taking a list of categories, labels and training set\"\"\"\n",
    "    length = {}\n",
    "    for cat in category:\n",
    "        length[\"length_\"+cat] = 0\n",
    "    for sent in t_train_set:\n",
    "        for cat in category:\n",
    "            if labels[t_train_set.index(sent)] == cat:\n",
    "                length[\"length_\"+cat] += len(sent)\n",
    "    return length\n",
    "    \n",
    "\n",
    "length_dict = category_length(categories, separ2, ytrain)\n",
    "    \n",
    "#count probabilities\n",
    "def words_probability(model, category, cat_length, vocabulary):\n",
    "    \"\"\" calculates probability in the given model with quantities of appearance of each word. \n",
    "    Input is model, a list of categories, number of words in each category and vocabulary of the training set\"\"\"\n",
    "    for word in model:\n",
    "        for c in model[word]:\n",
    "            for cat in category:\n",
    "                if cat == c:\n",
    "                    model[word][c] /= cat_length[\"length_\"+cat]+vocabulary\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_ent = words_probability(model_ent, categories, length_dict, length_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "cb7c0cf5-400c-4c89-8f7d-a2ee35e850fc",
    "deepnote_cell_height": 1107,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1647899963095,
    "source_hash": "bbb2b870",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 4. calculate given probabilities P(w|c) = count(w|c)+1 / counts(w+c) + vocab\n",
    "\n",
    "# create template\n",
    "def template(cat, bag_words):\n",
    "    \"\"\" input is a list with categories and unique words in a test set returns the dictionary \n",
    "    for further calculations of probabilities \"\"\"\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for i in bag_words:\n",
    "        for c in cat:\n",
    "            model[i][c] = 1 # add one for smoothing instead of 0\n",
    "    return model\n",
    "\n",
    "model_ent = template(categories, final_test)\n",
    "\n",
    "# count appearance \n",
    "def words_quantity(categories, bag_words, labels, t_train_set, model):\n",
    "    \"\"\" takes template model, categories, unique words in a test set, labels, and training test\n",
    "    to return the model with quantities of each word by category\"\"\"\n",
    "    for i in bag_words:\n",
    "        for cat in categories:\n",
    "            for sent in t_train_set:\n",
    "                if labels[t_train_set.index(sent)] == cat:\n",
    "                    for word in sent:\n",
    "                        if word == i:\n",
    "                            model[i][cat] += 1\n",
    "    return model\n",
    "\n",
    "model_ent = words_quantity(categories, final_test, ytrain, separ2, model_ent)\n",
    "\n",
    "# count number of words in each category\n",
    "def category_length(category, t_train_set, labels):\n",
    "    \"\"\" creates a dictionary where a key is length + category and value is number of words \n",
    "    in each category by taking a list of categories, labels and training set\"\"\"\n",
    "    length = {}\n",
    "    for cat in category:\n",
    "        length[\"length_\"+cat] = 0\n",
    "    for sent in t_train_set:\n",
    "        for cat in category:\n",
    "            if labels[t_train_set.index(sent)] == cat:\n",
    "                length[\"length_\"+cat] += len(sent)\n",
    "    return length\n",
    "    \n",
    "\n",
    "length_dict = category_length(categories, separ2, ytrain)\n",
    "    \n",
    "#count probabilities\n",
    "def words_probability(model, category, cat_length, vocabulary):\n",
    "    \"\"\" calculates probability in the given model with quantities of appearance of each word. \n",
    "    Input is model, a list of categories, number of words in each category and vocabulary of the training set\"\"\"\n",
    "    for word in model:\n",
    "        for c in model[word]:\n",
    "            for cat in category:\n",
    "                if cat == c:\n",
    "                    model[word][c] /= cat_length[\"length_\"+cat]+vocabulary\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_ent = words_probability(model_ent, categories, length_dict, length_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "775f77363c9c4891af06c523215a57ae",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1647899965443,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2abc63bb-dda8-43c2-995b-f7ae2a0ae9ba' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3e06faf3-ba55-4a75-8523-2f99b34502e8",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
